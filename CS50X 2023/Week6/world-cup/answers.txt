Times:

10 simulations: 0m0.030s
100 simulations: 0m0.032s
1000 simulations: 0m0.038s
10000 simulations: 0m0.100s
100000 simulations: 0m0.776s
1000000 simulations: 0m7.377s

Questions:

Which predictions, if any, proved incorrect as you increased the number of simulations?:

ANS: I predicted 100 and 1000 to have a slightly larger gap in runtime, perhaps by 0.040s but they turned out to be closer.

Suppose you're charged a fee for each second of compute time your program uses.
After how many simulations would you call the predictions "good enough"?:

ANS: The decision of 'good enough' depends on (1) achieving an satisfactory level of accuracy, which can be improves with a higher number of simulation runs,
while also (2) considering cost efficiency by minimizing runtime.

The runtimes for 10, 100, and 1000 simulations are relatively similar. Whereas, simulations with runtimes exceeding 1000 runs have higher varying runtimes, resulting in higher costs.

It's evident that the predictions stabilize with minumal changes around 1000 simulations.

I conclude that executing 1000 simulations is good enough as it best balances between providing a satisfactory accuracy level and maintaining low costs.




